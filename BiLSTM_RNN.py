# -*- coding: utf-8 -*-
"""BiLSTM_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f6LQhYQy-Yk00N75Z9M09Bp_s0HwvH1V
"""

from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense, Bidirectional
from keras.layers import LSTM
from keras.layers import Dropout
from keras.regularizers import l2
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import tensorflow as tf
# fix random seed for reproducibility
np.random.seed(7)

def read_data_small():
    X_train = pd.read_csv("X_train_small.csv")
    X_test = pd.read_csv("X_test_small.csv")
    y_train = np.asarray(pd.read_csv("y_train_small.csv", header=None)[0])
    return X_train, X_test, y_train


from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import scale
from sklearn.model_selection import cross_validate


### code classifier here ###
def format_data(df):
    # append numberical columns
    rst = df.loc[:, ["price", "volume", "bestBid", "bestAsk", 'bestBidVolume',
                     'bestAskVolume', 'lv2Bid', 'lv2BidVolume', 'lv2Ask',
                     'lv2AskVolume', 'lv3Bid', 'lv3BidVolume', 'lv3Ask',
                     'lv3AskVolume']]

    # encode the binaries
    rst["isBid"] = df.isBid * 1
    rst["isBuyer"] = df.isBuyer * 1
    rst["isAggressor"] = df.isAggressor * 1
    rst["type"] = (df.type == "ORDER") * 1
    rst["source"] = (df.source == "USER") * 1

    # parse the order id data
    rst["orderId"] = df.orderId.str.split('-').str[-1]
    rst["tradeId"] = df.tradeId.str.split('-').str[-1]
    rst["bidOrderId"] = df.bidOrderId.str.split('-').str[-1]
    rst["askOrderId"] = df.askOrderId.str.split('-').str[-1]

    # encode the multiple lable data
    tmp_operation = pd.DataFrame(pd.get_dummies(df.operation), columns=df.operation.unique()[:-1])
    rst = pd.concat([rst, tmp_operation], axis=1)
    tmp_endUserRef = pd.DataFrame(pd.get_dummies(df.endUserRef), columns=df.endUserRef.unique()[:-1])
    rst = pd.concat([rst, tmp_endUserRef], axis=1)

    # also feel free to add more columns inferred from data
    # smartly engineered features can be very useful to improve the classification resutls
    rst["timeSinceLastTrade"] = X_train[["timestamp", "endUserRef"]].groupby("endUserRef").diff()

    return rst

X_train, _ , Y_train = read_data_small()
X_clean_train = format_data(X_train)
X_clean_train = X_clean_train.fillna(-1)
x_scalers = X_clean_train.loc[:, ["price", "volume", "bestBid", "bestAsk", 'bestBidVolume',
                     'bestAskVolume', 'lv2Bid', 'lv2BidVolume', 'lv2Ask',
                     'lv2AskVolume', 'lv3Bid', 'lv3BidVolume', 'lv3Ask',
                     'lv3AskVolume']]
x_scalers = scale(x_scalers)
X_clean_train.drop(columns=["price", "volume", "bestBid", "bestAsk", 'bestBidVolume',
                     'bestAskVolume', 'lv2Bid', 'lv2BidVolume', 'lv2Ask',
                     'lv2AskVolume', 'lv3Bid', 'lv3BidVolume', 'lv3Ask',
                     'lv3AskVolume'], inplace=True)
x_scalers = pd.DataFrame(data=x_scalers, columns=["price", "volume", "bestBid", "bestAsk", 'bestBidVolume',
                     'bestAskVolume', 'lv2Bid', 'lv2BidVolume', 'lv2Ask',
                     'lv2AskVolume', 'lv3Bid', 'lv3BidVolume', 'lv3Ask',
                     'lv3AskVolume'])
x_clean_train = pd.concat([x_scalers, X_clean_train], axis=1)
x_train, x_test, y_train, y_test = train_test_split(x_clean_train, Y_train)
x_train = x_train.to_numpy()
x_test = x_test.to_numpy()
x_train.shape = x_train.shape[0], 1, x_train.shape[1]
x_test.shape = x_test.shape[0], 1, x_test.shape[1]
y_train = np.array([1 if i > 0 else 0 for i in y_train])

from sklearn import decomposition, datasets
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import scale
from sklearn.model_selection import cross_validate


pca = decomposition.PCA(n_components=6)

# Fit the PCA and transform the data
X_std_pca = pca.fit_transform(x_train)
X_test_pca = pca.transform(x_test)
clf = LogisticRegression(solver='lbfgs', random_state=0, class_weight='balanced', max_iter=1000000).fit(X_std_pca, y_train)
y_train_prob_pred = clf.predict_proba(X_std_pca)
y_test_prob_pred = clf.predict_proba(X_test_pca)



# truncate and pad input sequences
# create the model
model = Sequential()
model.add(Bidirectional(LSTM(128, input_shape=(106366, 497))))
model.add(Dropout(0.2))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.25, class_weight={0:0.004, 1:0.996})
print(model.summary())
# Final evaluation of the model
scores = model.evaluate(x_test, y_test, verbose=1)
print("Accuracy: %.2f%%" % (scores[1]*100))

results = model.predict(x_test)
for x, y in zip(results, y_test):
  if y != 0:
    print(x, x[0] > .5 , y)

from sklearn.metrics import cohen_kappa_score

def score(y_pred, y_true):
    """
    y_pred: a numpy 4d array of probabilities of point assigned to each label
    y_true: a numpy array of true labels
    """
    y_pred_label = np.argmax(y_pred, axis=1)
    return cohen_kappa_score(y_pred_label, y_true)
score(results, y_test)